[net]
# Testing #推理时才用的到Testing
# batch=1
# subdivisions=1
# Training #训练时使用以下参数
batch=64 #每次载入的批次大小，这里是每次载入64张图片
subdivisions=16 #这里将batch张图片分为16次送入，为了避免显存不够导致溢出
width=608 #训练时的图片宽度
height=608 #训练时的图片高度
#这里注意：训练集中的图片可以是各种尺度的，darknet框架下的训练会自动将图片resize成此大小
#也可以提前自己resize再作为训练集，这样训练速度会快一些
channels=3 #三通道的图像
momentum=0.9 #动量，影响梯度下降的速度
decay=0.0005 #权重衰减比，该值越大，模型抑制过拟合的能力越强，但过大会导致模型无法收敛
angle=15 #样本增强项-角度，会随机将样本在-15度到15度之间旋转来增多样本
saturation = 1.5 #样本增强项-饱和度，目的同样是增多样本
exposure = 1.5 #样本增强项-曝光度，目的同上
hue=.1 #样本增强项-色调，目的同上
 
learning_rate=0.001 #初始学习率
burn_in=1000 #训练次数少于burn_in次时，学习率不变，多于burn_in次时采用policy的学习率更新策略
max_batches = 500200 #最大训练的batch数，超过该数就停止训练
policy=steps #使用policy的训练策略，训练策略有:CONSTANT, STEP, EXP, POLY, STEPS, SIG, RANDOM这几种
steps=300000,450000 #训练次数达到300000次时，学习率衰减第一个scale倍(10倍)，450000次时同样衰减10倍
scales=.1,.1 #与steps配合
 
[convolutional]
batch_normalize=1 #是否使用bn层，1代表是
filters=32 #该卷积层滤波器的个数
size=3 #滤波器大小，这里是3x3
stride=1 #滑动步长
pad=1 #是否需要padding补0
activation=leaky #激活函数，leaky_Relu
 
# Downsample 下采样过程，这里代表下一层卷积层运算过后，特征图的x，y维度会变为原来的stride倍
 
[convolutional]
batch_normalize=1
filters=64
size=3
stride=2 #滑动步长为2，这里实现了下采样
pad=1
activation=leaky
 
[convolutional]
batch_normalize=1
filters=32
size=1
stride=1
pad=1
activation=leaky
 
[convolutional]
batch_normalize=1
filters=64
size=3
stride=1
pad=1
activation=leaky
 
[shortcut]
from=-3
activation=linear
 
# Downsample
 
[convolutional]
batch_normalize=1
filters=128
size=3
stride=2
pad=1
activation=leaky
 
[convolutional]
batch_normalize=1
filters=64
size=1
stride=1
pad=1
activation=leaky
 
[convolutional]
batch_normalize=1
filters=128
size=3
stride=1
pad=1
activation=leaky
 
[shortcut]
from=-3
activation=linear
 
[convolutional]
batch_normalize=1
filters=64
size=1
stride=1
pad=1
activation=leaky
 
[convolutional]
batch_normalize=1
filters=128
size=3
stride=1
pad=1
activation=leaky
 
[shortcut]
from=-3
activation=linear
 
# Downsample
 
[convolutional]
batch_normalize=1
filters=256
size=3
stride=2
pad=1
activation=leaky
 
[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky
 
[convolutional]
batch_normalize=1
filters=256
size=3
stride=1
pad=1
activation=leaky
 
[shortcut]
from=-3
activation=linear
 
[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky
 
[convolutional]
batch_normalize=1
filters=256
size=3
stride=1
pad=1
activation=leaky
 
[shortcut]
from=-3
activation=linear
 
[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky
 
[convolutional]
batch_normalize=1
filters=256
size=3
stride=1
pad=1
activation=leaky
 
[shortcut]
from=-3
activation=linear
 
[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky
 
[convolutional]
batch_normalize=1
filters=256
size=3
stride=1
pad=1
activation=leaky
 
[shortcut]
from=-3
activation=linear
 
 
[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky
 
[convolutional]
batch_normalize=1
filters=256
size=3
stride=1
pad=1
activation=leaky
 
[shortcut]
from=-3
activation=linear
 
[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky
 
[convolutional]
batch_normalize=1
filters=256
size=3
stride=1
pad=1
activation=leaky
 
[shortcut]
from=-3
activation=linear
 
[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky
 
[convolutional]
batch_normalize=1
filters=256
size=3
stride=1
pad=1
activation=leaky
 
[shortcut]
from=-3
activation=linear
 
[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky
 
[convolutional]
batch_normalize=1
filters=256
size=3
stride=1
pad=1
activation=leaky
 
[shortcut]
from=-3
activation=linear
 
# Downsample
 
[convolutional]
batch_normalize=1
filters=512
size=3
stride=2
pad=1
activation=leaky
 
[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky
 
[convolutional]
batch_normalize=1
filters=512
size=3
stride=1
pad=1
activation=leaky
 
[shortcut]
from=-3
activation=linear
 
 
[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky
 
[convolutional]
batch_normalize=1
filters=512
size=3
stride=1
pad=1
activation=leaky
 
[shortcut]
from=-3
activation=linear
 
 
[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky
 
[convolutional]
batch_normalize=1
filters=512
size=3
stride=1
pad=1
activation=leaky
 
[shortcut]
from=-3
activation=linear
 
 
[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky
 
[convolutional]
batch_normalize=1
filters=512
size=3
stride=1
pad=1
activation=leaky
 
[shortcut]
from=-3
activation=linear
 
[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky
 
[convolutional]
batch_normalize=1
filters=512
size=3
stride=1
pad=1
activation=leaky
 
[shortcut]
from=-3
activation=linear
 
 
[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky
 
[convolutional]
batch_normalize=1
filters=512
size=3
stride=1
pad=1
activation=leaky
 
[shortcut]
from=-3
activation=linear
 
 
[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky
 
[convolutional]
batch_normalize=1
filters=512
size=3
stride=1
pad=1
activation=leaky
 
[shortcut]
from=-3
activation=linear
 
[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky
 
[convolutional]
batch_normalize=1
filters=512
size=3
stride=1
pad=1
activation=leaky
 
[shortcut]
from=-3
activation=linear
 
# Downsample
 
[convolutional]
batch_normalize=1
filters=1024
size=3
stride=2
pad=1
activation=leaky
 
[convolutional]
batch_normalize=1
filters=512
size=1
stride=1
pad=1
activation=leaky
 
[convolutional]
batch_normalize=1
filters=1024
size=3
stride=1
pad=1
activation=leaky
 
[shortcut]
from=-3
activation=linear
 
[convolutional]
batch_normalize=1
filters=512
size=1
stride=1
pad=1
activation=leaky
 
[convolutional]
batch_normalize=1
filters=1024
size=3
stride=1
pad=1
activation=leaky
 
[shortcut]
from=-3
activation=linear
 
[convolutional]
batch_normalize=1
filters=512
size=1
stride=1
pad=1
activation=leaky
 
[convolutional]
batch_normalize=1
filters=1024
size=3
stride=1
pad=1
activation=leaky
 
[shortcut]
from=-3
activation=linear
 
[convolutional]
batch_normalize=1
filters=512
size=1
stride=1
pad=1
activation=leaky
 
[convolutional]
batch_normalize=1
filters=1024
size=3
stride=1
pad=1
activation=leaky
 
[shortcut]
from=-3
activation=linear
 
###################### 从这里开始，三个yolo层共用的网络结构到此结束
 
[convolutional]
batch_normalize=1
filters=512
size=1
stride=1
pad=1
activation=leaky
 
[convolutional]
batch_normalize=1
size=3
stride=1
pad=1
filters=1024
activation=leaky
 
[convolutional]
batch_normalize=1
filters=512
size=1
stride=1
pad=1
activation=leaky
 
[convolutional]
batch_normalize=1
size=3
stride=1
pad=1
filters=1024
activation=leaky
 
[convolutional]
batch_normalize=1
filters=512
size=1
stride=1
pad=1
activation=leaky
 
[convolutional]
batch_normalize=1
size=3
stride=1
pad=1
filters=1024
activation=leaky
 
[convolutional]
size=1
stride=1
pad=1
filters=18
activation=linear
 
 
[yolo]
mask = 6,7,8 #表示该层yolo层选用那几个Anchor
anchors = 10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326
#整个yolov3网络使用了9个Ancnor尺寸，都在这里，通过mask来选择该层yolo层选用哪几个anchor。
#anchor是利用k-means算法基于训练集而得到的目标统计尺寸。本层选用了最大的三个anchor，很显然，本层的目的是着眼于检测大目标。
classes=1 #训练集的类别数
num=9 #anchor数目
jitter=.3 #利用数据抖动来产生更多的数据，这里的抖动概率是0.3
ignore_thresh = .7 
#当预测框与真实框（ground truth）的IOU超过该值时，不参与loss计算，否则参与计算
truth_thresh = 1
random=1#如果为1，每次迭代图片大小随机从320到608，步长为32，如果为0，每次训练大小与输入大小一致。也就是多尺度训练。
 
 
[route]
layers = -4
#用于特征融合，这里表示从上一层开始数的倒数第4层作为本层。
 
[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky
 
[upsample]
stride=2 #上采样过程，特征图的x，y维度变为上一层的2倍
 
[route]
layers = -1, 61
#特征融合，倒数第1层（上一层开始算）与第61层融合，也就是滤波器直接叠加。
 
 
[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky
 
[convolutional]
batch_normalize=1
size=3
stride=1
pad=1
filters=512
activation=leaky
 
[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky
 
[convolutional]
batch_normalize=1
size=3
stride=1
pad=1
filters=512
activation=leaky
 
[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky
 
[convolutional]
batch_normalize=1
size=3
stride=1
pad=1
filters=512
activation=leaky
 
[convolutional]
size=1
stride=1
pad=1
filters=18
activation=linear
 
 
[yolo]
mask = 3,4,5
anchors = 10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326
classes=1
num=9
jitter=.3
ignore_thresh = .7
truth_thresh = 1
random=1
 
 
 
[route]
layers = -4
 
[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky
 
[upsample]
stride=2
 
[route]
layers = -1, 36
 
 
 
[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky
 
[convolutional]
batch_normalize=1
size=3
stride=1
pad=1
filters=256
activation=leaky
 
[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky
 
[convolutional]
batch_normalize=1
size=3
stride=1
pad=1
filters=256
activation=leaky
 
[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky
 
[convolutional]
batch_normalize=1
size=3
stride=1
pad=1
filters=256
activation=leaky
 
[convolutional]
size=1
stride=1
pad=1
filters=18
activation=linear
 
 
[yolo]
mask = 0,1,2
anchors = 10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326
classes=1
num=9
jitter=.3
ignore_thresh = .7
truth_thresh = 1
random=1